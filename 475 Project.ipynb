{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 475 Project - Creating a Trailer for a Musical Album\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using various methods such as key detection, tempo matching, and musical structure analysis, we will create a software that takes an album as input, and returns a single cohesive song that is created from segments of each song from the inputted set. Our project will choose sections by analyzing each song and determining verses and chorus sections in order to choose the portion that is to be added into the outputted song. Once these sections are determined, they will be stitched together into a single song by various possible methods, including fading in/out, downbeat repetition, or hard cuts. The order that the segments are placed in is yet to be determined, whether it be in the same album tracklist ordering, or an order that is deemed to be the most smooth for transitions between song segments. In order to create smooth transitions between song segments, the tempo and key of each segment must be detected in order to be able to adjust properly during transitions.\n",
    "\n",
    "When searching for a new artist, many listeners choose to listen to the singles released preceding an album drop to determine whether or not listening to an album is worth their time. With the rising popularity of Spotify recommendation algorithms and curated playlists, it is easy to see the demand for a song that gives the listener a short preview of an album without requiring to listen to it front to back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import pychorus as pc\n",
    "import vamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Variable Entry & Directory Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"albums/R&B/Malibu\"\n",
    "new_file_name = \"AlbumTrailerMalibu.wav\" #wav file\n",
    "\n",
    "# how many beats the segment lasts after segment start time\n",
    "num_beats = 16\n",
    "\n",
    "# would you like to timestretch files to master tempo?\n",
    "timestretch_segs = False\n",
    "\n",
    "# would you like to reorder tracks according to tempo? Default: Slow to Fast\n",
    "reorder_tracks = True\n",
    "\n",
    "# order them from Fast To Slow instead\n",
    "fast_to_slow = False\n",
    "\n",
    "#Fade and delay times between files (in seconds)\n",
    "cross_fade = 2\n",
    "init_fade = 1 \n",
    "final_fade = 2\n",
    "delay = 1 # should be less than cross_fade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "srates = []\n",
    "audio_files = []\n",
    "srate = 0\n",
    "\n",
    "#Chorus/Beat Detection\n",
    "hopSize = 512\n",
    "song_data = {}\n",
    "beats = []\n",
    "\n",
    "#Splitting and Crossfades\n",
    "aud = []\n",
    "trailer = []\n",
    "prevl = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "for i, filename in enumerate(glob.glob(os.path.join(path, '*.wav'))):\n",
    "    data, s = librosa.load(filename)\n",
    "    srates.append(s)\n",
    "    audio_files.append(data)\n",
    "    # Scale to -1.0/1.0\n",
    "    audio_files[i] = audio_files[i].astype(np.float32) / 32767.0\n",
    "    # Make max be 0.9\n",
    "    audio_files[i] = ((0.9 / np.max(audio_files[i])) * audio_files[i])\n",
    "    \n",
    "    # Handle when files have different sampling rates\n",
    "    if (i != 0):\n",
    "        if (srates[i] != srates[i-1]):\n",
    "            raise ValueError(\"Mismatched sampling rates between file {} and {}\".format(i-1, i))\n",
    "            \n",
    "    # If all srates are the same, simplify to single value variable\n",
    "    srate = srates[i]\n",
    "\n",
    "\n",
    "titles = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
    "num_files = len(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beat Tracking & Chorus Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what i've done below is created a dictionary with data of each song (song_data).\n",
    "this contains (for now):\n",
    "chorus_starttime: start of a chorus timestamp. This value is in seconds\n",
    "tempo: this is in bpm\n",
    "length: length of the song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No choruses were detected. Try a smaller search duration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pairs = []\n",
    "labels = []\n",
    "\n",
    "for i, filename in enumerate(glob.glob(os.path.join(path, '*.wav'))):\n",
    "    #finds the chorus_start\n",
    "    chroma, y, sr, sec = pc.create_chroma(filename)\n",
    "    chorus_start = pc.find_chorus(chroma, sr, sec, 10)\n",
    "    if(isinstance(chorus_start, float)):\n",
    "        pass\n",
    "        # chorus_start worked\n",
    "    else:\n",
    "        segments = vamp.process_audio(y, sr, \"segmentino:segmentino\")\n",
    "        # Get labels and timestamps for each section\n",
    "        for s in segments:\n",
    "            pairs.append(tuple((s['label'], s['timestamp'])))\n",
    "        \n",
    "        # Get list of just labels (for counting purposes)\n",
    "        for p in pairs:\n",
    "            labels.append(p[0])\n",
    "            \n",
    "        # Get most common label\n",
    "        m = max(set(labels), key=labels.count)\n",
    "        \n",
    "        # Get chorus start\n",
    "        chorus_start = pairs[labels.index(m)][1]\n",
    "        \n",
    "    # finds the tempo and song length\n",
    "    tempo, beat_times = librosa.beat.beat_track(audio_files[i], sr=srate, hop_length=hopSize, start_bpm=70, units='time')\n",
    "    if (tempo < 60):\n",
    "        tempo = tempo*2\n",
    "    if (tempo > 140):\n",
    "        tempo = tempo/2\n",
    "    song_length = len(audio_files[i])/srate\n",
    "    # beat times are an array of times that beat tracker has located\n",
    "    beats.append(beat_times)\n",
    "    \n",
    "    song_data[i] = {\n",
    "        'length': song_length,\n",
    "        'tempo' : tempo,\n",
    "        'chorus_starttime': chorus_start\n",
    "    }        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if tempos have been registered\n",
    "tempos_exist = True\n",
    "for i in range(num_files):\n",
    "    if(song_data[i].get('tempo')==0):\n",
    "        tempos_exist = False;\n",
    "\n",
    "if(tempos_exist == False):\n",
    "    timestretch_segs = False\n",
    "    reorder_tracks = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting at Chorus, Crossfading, and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts a time in seconds to a specific sample\n",
    "def sec_convert(seconds):\n",
    "    samples = seconds*srate\n",
    "    return round(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the first beat after chorus_start, and a beat num_beats after \n",
    "# if first_beat + num_beats is past the last beat, choose the very last one\n",
    "# converts these to a sample value\n",
    "def choose_first_last_beat(timestamp, num_beats, songindex):\n",
    "\n",
    "    for i, x in enumerate(beats[songindex]):\n",
    "        if (timestamp+3 > x > timestamp):\n",
    "            try:\n",
    "                # +1 because you want the end of the last of num_beats\n",
    "                last_beat = beats[songindex][i+num_beats]\n",
    "            except IndexError:\n",
    "                last_beat = beats[songindex][-1]\n",
    "\n",
    "            return x, last_beat\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    #found no suitable beats so choose a time duration instead (~22 seconds)\n",
    "    last_beat = timestamp+(num_beats*1.5)\n",
    "        \n",
    "    return timestamp, last_beat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Chorus, takes song_info array, int i, signal array x, and int num_beats\n",
    "def chorus(s, i, x, b):\n",
    "    start_time = s[i].get('chorus_starttime')\n",
    "    first_beat, last_beat = choose_first_last_beat(start_time, b, i)\n",
    "    first_sample = int(sec_convert(first_beat))\n",
    "    last_sample = int(sec_convert(last_beat))\n",
    "    try:\n",
    "        c = x[first_sample:last_sample]\n",
    "    except IndexError:\n",
    "            last_sample = x[-1]\n",
    "            c = x[first_sample:last_sample]\n",
    "    \n",
    "    return c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CrossFading Function, takes signal arrays s1, s2, and fadetime f and delay time d in samples\n",
    "def crossfade(s1,s2,f,d):\n",
    "    \n",
    "    #Add padded 0's to delay overlap of signals\n",
    "    a = np.pad(s1[(len(s1)-f):], (0, d), 'constant', constant_values=(0, 0))\n",
    "    b = np.pad(s2[:f], (d, 0), 'constant', constant_values=(0, 0))\n",
    "    c = []\n",
    "    l = f+d\n",
    "    \n",
    "    #Fade out s1 and fade in s2\n",
    "    for i in range(0, l):\n",
    "        m = i/f\n",
    "        a[i] = a[i]*(1-m) #Decreases from 1 to 0 over fade duration\n",
    "        b[(l-1)-i] = b[(l-1)-i]*(1-m) #Increase from 0 to 1 over fade duration\n",
    "        \n",
    "    a = a + b #Overlap both faded signals\n",
    "    c = np.concatenate((s1[:len(s1)-f],a,s2[f:]), axis=0)\n",
    "   \n",
    "    #For testing\n",
    "    #ipd.display(ipd.Audio(c,rate=srate))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determines if Fade length is longer than half the duration of the segment, \n",
    "#and corrects to .75*l/2 if it is too long\n",
    "def det_fade (f, l):\n",
    "    if (f >= (l/2)):\n",
    "        newf = int(0.75 * (l/2))\n",
    "    else:\n",
    "        newf = f   \n",
    "    return newf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Segmenting/Fading Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a list of tempos for the tracks and a mean tempo\n",
    "if(timestretch_segs == True or reorder_tracks == True):\n",
    "    tempo_list = []\n",
    "    for i in range(num_files):\n",
    "        tempo_list.append(song_data[i].get('tempo'))\n",
    "\n",
    "    mean_tempo = np.mean(tempo_list)\n",
    "    #print(mean_tempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the scaling value for the timestretch function\n",
    "def timestretch_value(mean_tempo, tempo):\n",
    "    value = mean_tempo/tempo\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stretches the segments accordingly and places them in an array\n",
    "if(timestretch_segs == True or reorder_tracks == True):\n",
    "    timestretched_segments = []\n",
    "    for i, x in enumerate(audio_files):\n",
    "        audio = chorus(song_data, i, x, num_beats)\n",
    "        stretch_value = timestretch_value(mean_tempo, song_data[i].get('tempo'))\n",
    "        timestretched_segments.append(librosa.effects.time_stretch(audio, stretch_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION OVERWRITES THE TEMPO_LIST CREATED EARLIER\n",
    "# reorders segments according to their tempo by chosen ordering\n",
    "if(timestretch_segs == True or reorder_tracks == True):\n",
    "    reordered_segments = []\n",
    "    for x in audio_files:\n",
    "        if(fast_to_slow == True):\n",
    "            index = tempo_list.index(max(tempo_list))\n",
    "        else:\n",
    "            index = tempo_list.index(min(tempo_list))\n",
    "            \n",
    "        if(fast_to_slow == True):\n",
    "            tempo_list[index] = -1\n",
    "        else:\n",
    "            tempo_list[index] = 200\n",
    "        segment = chorus(song_data, index, x, num_beats)\n",
    "        reordered_segments.append(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmenting Trailer and adding Fade in/out and Crossfades\n",
    "crsf = sec_convert(cross_fade)\n",
    "inif = sec_convert(init_fade)\n",
    "finf = sec_convert(final_fade)\n",
    "d = sec_convert(delay)\n",
    "\n",
    "for i, x in enumerate(audio_files):\n",
    "    #Pull Track Segment\n",
    "    if timestretch_segs == True:\n",
    "        aud = timestretched_segments[i]\n",
    "    elif reorder_tracks == True:\n",
    "        aud = reordered_segments[i]\n",
    "    else:\n",
    "        aud = chorus(song_data, i, x, num_beats)\n",
    "    l = len(aud)\n",
    "\n",
    "    ### First Track ###\n",
    "    if (i == 0):\n",
    "        #print(\"First Track 1\")\n",
    "        #Determine if Initial Fade is too long\n",
    "        f = det_fade(inif, l)\n",
    "        \n",
    "        #add fade in to first track\n",
    "        for k in range(0, f):\n",
    "            m = k /f\n",
    "            aud[k] = aud[k]*m\n",
    "        \n",
    "        trailer = aud\n",
    "        prevl = l \n",
    "    \n",
    "    ### last Track ###\n",
    "    elif (i == num_files-1):\n",
    "        #print(\"Last Track %i\" % (i+1))\n",
    "        #Determine if Final Fade is too long\n",
    "        f = det_fade(finf, l)\n",
    "            \n",
    "        #Determine if Crossfade is too long\n",
    "        f2 = det_fade(crsf, l) #new segment\n",
    "        f2 = det_fade(f2, prevl) #prev segment\n",
    "        \n",
    "        #add fade out to last track\n",
    "        for k in range(l-f, l):\n",
    "            m = (l - k)/f\n",
    "            aud[k] = aud[k]*m\n",
    "       \n",
    "        trailer = crossfade(trailer,aud,f2,d)\n",
    "        \n",
    "    ### Middle Tracks ###\n",
    "    else:\n",
    "        #print(\"Track %i\" % (i+1))\n",
    "        #Determine if Crossfade is too long\n",
    "        f = det_fade(crsf, l) #new segment\n",
    "        f = det_fade(f, prevl) #prev segment\n",
    "        \n",
    "        trailer = crossfade(trailer,aud,f,d)\n",
    "        prevl = l\n",
    "\n",
    "#Convert data type back so that conversion to wav file works\n",
    "trailer = np.float32(trailer)\n",
    "\n",
    "#For Testing\n",
    "#ipd.display(ipd.Audio(trailer,rate=srate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.write(new_file_name, int(srate), np.array(trailer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
